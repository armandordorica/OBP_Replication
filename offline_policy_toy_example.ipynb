{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "512d406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armandoordoricadelatorre/Documents/U of T/PhD/PhD Research/OBP_Replication\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import importlib\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "from itertools import product\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# OBP imports\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.ope import (\n",
    "    DirectMethod as DM_OBP,\n",
    "    DoublyRobust as DR_OBP,\n",
    "    InverseProbabilityWeighting as IPW,\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel\n",
    ")\n",
    "from obp.policy import IPWLearner\n",
    "\n",
    "# Local imports\n",
    "import stats\n",
    "import visualizations\n",
    "importlib.reload(stats)\n",
    "importlib.reload(visualizations)\n",
    "from visualizations import (\n",
    "    plot_bar_chart,\n",
    "    plot_boxplot_with_stats,\n",
    "    plot_distribution_with_cdf,\n",
    "    plot_histogram_with_stats\n",
    ")\n",
    "from stats import (\n",
    "    calculate_distribution_stats,\n",
    "    compute_feature_combinations,\n",
    "    compute_item_feature_distribution,\n",
    "    compute_item_propensity_stats,\n",
    "    compute_manual_propensity,\n",
    "    compute_propensity_variance\n",
    ")\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a601ab8",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe189c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remap_user_features(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Map hash values in user_feature_N to short readable codes like A1, B1, ...\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    mapping_dicts = {}\n",
    "\n",
    "    for col in feature_cols:\n",
    "        # Extract the feature index (N from 'user_feature_N')\n",
    "        feature_idx = col.split(\"_\")[-1]\n",
    "        uniques = df[col].dropna().unique()\n",
    "\n",
    "        # Build codes A{N}, B{N}, C{N}...\n",
    "        codes = [f\"{letter}{feature_idx}\" for letter in string.ascii_uppercase[:len(uniques)]]\n",
    "        mapping = dict(zip(uniques, codes))\n",
    "\n",
    "        df_copy[col] = df[col].map(mapping)\n",
    "        mapping_dicts[col] = mapping\n",
    "\n",
    "    return df_copy, mapping_dicts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b881c",
   "metadata": {},
   "source": [
    "### Import logged data from `all.csv` and `item_context.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0680605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_df shape: (10000, 89)\n",
      "items_df shape: (80, 5)\n",
      "\n",
      "log_df columns:\n",
      "['timestamp', 'item_id', 'position', 'click', 'propensity_score', 'user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3', 'user-item_affinity_0', 'user-item_affinity_1', 'user-item_affinity_2', 'user-item_affinity_3', 'user-item_affinity_4', 'user-item_affinity_5', 'user-item_affinity_6', 'user-item_affinity_7', 'user-item_affinity_8', 'user-item_affinity_9', 'user-item_affinity_10', 'user-item_affinity_11', 'user-item_affinity_12', 'user-item_affinity_13', 'user-item_affinity_14', 'user-item_affinity_15', 'user-item_affinity_16', 'user-item_affinity_17', 'user-item_affinity_18', 'user-item_affinity_19', 'user-item_affinity_20', 'user-item_affinity_21', 'user-item_affinity_22', 'user-item_affinity_23', 'user-item_affinity_24', 'user-item_affinity_25', 'user-item_affinity_26', 'user-item_affinity_27', 'user-item_affinity_28', 'user-item_affinity_29', 'user-item_affinity_30']\n",
      "\n",
      "first 5 log rows:\n",
      "                          timestamp  item_id  position  click  propensity_score                    user_feature_0                    user_feature_1                    user_feature_2                    user_feature_3  user-item_affinity_0  user-item_affinity_1  user-item_affinity_2  user-item_affinity_3  user-item_affinity_4  user-item_affinity_5  user-item_affinity_6  user-item_affinity_7  user-item_affinity_8  user-item_affinity_9  user-item_affinity_10  user-item_affinity_11  user-item_affinity_12  user-item_affinity_13  user-item_affinity_14  user-item_affinity_15  user-item_affinity_16  user-item_affinity_17  user-item_affinity_18  user-item_affinity_19  user-item_affinity_20  user-item_affinity_21  user-item_affinity_22  user-item_affinity_23  user-item_affinity_24  user-item_affinity_25  user-item_affinity_26  user-item_affinity_27  user-item_affinity_28  user-item_affinity_29  user-item_affinity_30  user-item_affinity_31  user-item_affinity_32  user-item_affinity_33  \\\n",
      "0  2019-11-24 00:00:17.004101+00:00       79         2      0          0.087125  81ce123cbb5bd8ce818f60fb3586bba5  03a5648a76832f83c859d46bc06cb64a  7bc94a2da491829b777c49c4b5e480f2  c39b0c7dd5d4eb9a18e7db6ba2f258f8                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "1  2019-11-24 00:00:19.715857+00:00       14         1      0          0.006235  81ce123cbb5bd8ce818f60fb3586bba5  2d03db5543b14483e52d761760686b64  2723d2eb8bba04e0362098011fa3997b  9bde591ffaab8d54c457448e4dca6f53                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "2  2019-11-24 00:01:04.303227+00:00       18         2      0          0.061300  81ce123cbb5bd8ce818f60fb3586bba5  03a5648a76832f83c859d46bc06cb64a  c2e4f76cdbabecd33b8c762aeef386b3  c39b0c7dd5d4eb9a18e7db6ba2f258f8                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "3  2019-11-24 00:01:11.571162+00:00       28         1      0          0.019430  81ce123cbb5bd8ce818f60fb3586bba5  03a5648a76832f83c859d46bc06cb64a  7bc94a2da491829b777c49c4b5e480f2  9bde591ffaab8d54c457448e4dca6f53                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "4  2019-11-24 00:02:41.811768+00:00       65         2      0          0.019375  81ce123cbb5bd8ce818f60fb3586bba5  03a5648a76832f83c859d46bc06cb64a  c2e4f76cdbabecd33b8c762aeef386b3  9bde591ffaab8d54c457448e4dca6f53                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "\n",
      "   user-item_affinity_34  user-item_affinity_35  user-item_affinity_36  user-item_affinity_37  user-item_affinity_38  user-item_affinity_39  user-item_affinity_40  user-item_affinity_41  user-item_affinity_42  user-item_affinity_43  user-item_affinity_44  user-item_affinity_45  user-item_affinity_46  user-item_affinity_47  user-item_affinity_48  user-item_affinity_49  user-item_affinity_50  user-item_affinity_51  user-item_affinity_52  user-item_affinity_53  user-item_affinity_54  user-item_affinity_55  user-item_affinity_56  user-item_affinity_57  user-item_affinity_58  user-item_affinity_59  user-item_affinity_60  user-item_affinity_61  user-item_affinity_62  user-item_affinity_63  user-item_affinity_64  user-item_affinity_65  user-item_affinity_66  user-item_affinity_67  user-item_affinity_68  user-item_affinity_69  user-item_affinity_70  user-item_affinity_71  user-item_affinity_72  user-item_affinity_73  user-item_affinity_74  user-item_affinity_75  user-item_affinity_76  \\\n",
      "0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "1                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "2                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "3                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "4                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
      "\n",
      "   user-item_affinity_77  user-item_affinity_78  user-item_affinity_79  \n",
      "0                    0.0                    0.0                    0.0  \n",
      "1                    0.0                    0.0                    0.0  \n",
      "2                    0.0                    0.0                    0.0  \n",
      "3                    0.0                    0.0                    0.0  \n",
      "4                    0.0                    0.0                    0.0  \n",
      "\n",
      "first 5 item rows:\n",
      "   item_id  item_feature_0                    item_feature_1                    item_feature_2                    item_feature_3\n",
      "0        0       -0.499172  527a325f46248058e3c5df9fb548f05f  5e92ce840abf12dc17408ca42b189836  84fd569b1b2309788c39aced8f88d084\n",
      "1        1       -0.543775  84c5238bc7dfedff64df3e71e39456d3  5e92ce840abf12dc17408ca42b189836  84fd569b1b2309788c39aced8f88d084\n",
      "2        2        0.972752  84c5238bc7dfedff64df3e71e39456d3  b913be25b9f7b764dcdeeac826fbd94b  78469ccb0d9544e55354632ff51ed422\n",
      "3        3       -0.521473  ab5678265d1b6b884ddc30cac35dc0b7  1c3a19a4f1fb7e108afd14f43393a79d  3c8f79fff6b73fd5a3eaebdc20b37078\n",
      "4        4        1.909430  31a4dca305b5aae308a77f24d2568951  8d83a426afeac570837704062f5bbe37  78469ccb0d9544e55354632ff51ed422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BTS / ALL sample\n",
    "log_df   = pd.read_csv(\"zr-obp/obd/bts/all/all.csv\", index_col=0)\n",
    "items_df = pd.read_csv(\"zr-obp/obd/bts/all/item_context.csv\", index_col=0)\n",
    "\n",
    "print(\"log_df shape:\", log_df.shape)\n",
    "print(\"items_df shape:\", items_df.shape)\n",
    "\n",
    "print(\"\\nlog_df columns:\")\n",
    "print(log_df.columns.tolist()[:40])  # peek first ~40 col names\n",
    "\n",
    "print(\"\\nfirst 5 log rows:\")\n",
    "print(log_df.head())\n",
    "\n",
    "print(\"\\nfirst 5 item rows:\")\n",
    "print(items_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fb8aa",
   "metadata": {},
   "source": [
    "### Remapping categorical features to readable categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "085e89eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample remapped features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>user_feature_1</th>\n",
       "      <th>user_feature_2</th>\n",
       "      <th>user_feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0</td>\n",
       "      <td>B1</td>\n",
       "      <td>B2</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_feature_0 user_feature_1 user_feature_2 user_feature_3\n",
       "0             A0             A1             A2             A3\n",
       "1             A0             B1             B2             B3\n",
       "2             A0             A1             C2             A3\n",
       "3             A0             A1             A2             B3\n",
       "4             A0             A1             C2             B3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings used:\n",
      "user_feature_0: {'81ce123cbb5bd8ce818f60fb3586bba5': 'A0', 'cef3390ed299c09874189c387777674a': 'B0', '4ae385d792f81dde128124a925a830de': 'C0'}\n",
      "user_feature_1: {'03a5648a76832f83c859d46bc06cb64a': 'A1', '2d03db5543b14483e52d761760686b64': 'B1', '6ff54aa8ff7a9dde75161c20a3ee4231': 'C1', 'f1c2d6a32ec39249160cf784b63f4c6f': 'D1', '8b50621825ffd909dd8d8317d366271f': 'E1'}\n",
      "user_feature_2: {'7bc94a2da491829b777c49c4b5e480f2': 'A2', '2723d2eb8bba04e0362098011fa3997b': 'B2', 'c2e4f76cdbabecd33b8c762aeef386b3': 'C2', '719dab53a7560218a9d1f96b25d6fa32': 'D2', '9b2d331c329ceb74d3dcfb48d8798c78': 'E2', '302deff13f835d731df1c842eed95971': 'F2', '9f4e8271d3d3014af5f35124c2de5082': 'G2', '7ae37150e596e6e8f19e27a06bd4d359': 'H2', 'c7cce49040b6630e9b5484dfcc0e6cd1': 'I2'}\n",
      "user_feature_3: {'c39b0c7dd5d4eb9a18e7db6ba2f258f8': 'A3', '9bde591ffaab8d54c457448e4dca6f53': 'B3', '05b76f5e97e51128862059ac7df9e42a': 'C3', 'f97571b9c14a786aab269f0b427d2a85': 'D3', '06128286bcc64b6a4b0fb7bc0328fe17': 'E3', '270b3e1c052b4f2e9c90bf0ebeb84f34': 'F3', 'ec6cfbbf6c92863522964288cddad06c': 'G3', '3aef83d337306549f1f8ca5a8b8ecff8': 'H3', '1f5491ca59138af653430c90c33319b5': 'I3'}\n"
     ]
    }
   ],
   "source": [
    "user_feature_cols = [c for c in log_df.columns if c.startswith(\"user_feature\")]\n",
    "log_df_readable, mappings = remap_user_features(log_df, user_feature_cols)\n",
    "\n",
    "print(\"Sample remapped features:\")\n",
    "display(log_df_readable[user_feature_cols].head())\n",
    "\n",
    "print(\"\\nMappings used:\")\n",
    "for feat, mapping in mappings.items():\n",
    "    print(f\"{feat}: {mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e2f11a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>position</th>\n",
       "      <th>click</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>user_feature_1</th>\n",
       "      <th>user_feature_2</th>\n",
       "      <th>user_feature_3</th>\n",
       "      <th>user-item_affinity_0</th>\n",
       "      <th>user-item_affinity_1</th>\n",
       "      <th>user-item_affinity_2</th>\n",
       "      <th>user-item_affinity_3</th>\n",
       "      <th>user-item_affinity_4</th>\n",
       "      <th>user-item_affinity_5</th>\n",
       "      <th>user-item_affinity_6</th>\n",
       "      <th>user-item_affinity_7</th>\n",
       "      <th>user-item_affinity_8</th>\n",
       "      <th>user-item_affinity_9</th>\n",
       "      <th>user-item_affinity_10</th>\n",
       "      <th>user-item_affinity_11</th>\n",
       "      <th>user-item_affinity_12</th>\n",
       "      <th>user-item_affinity_13</th>\n",
       "      <th>user-item_affinity_14</th>\n",
       "      <th>user-item_affinity_15</th>\n",
       "      <th>user-item_affinity_16</th>\n",
       "      <th>user-item_affinity_17</th>\n",
       "      <th>user-item_affinity_18</th>\n",
       "      <th>user-item_affinity_19</th>\n",
       "      <th>user-item_affinity_20</th>\n",
       "      <th>user-item_affinity_21</th>\n",
       "      <th>user-item_affinity_22</th>\n",
       "      <th>user-item_affinity_23</th>\n",
       "      <th>user-item_affinity_24</th>\n",
       "      <th>user-item_affinity_25</th>\n",
       "      <th>user-item_affinity_26</th>\n",
       "      <th>user-item_affinity_27</th>\n",
       "      <th>user-item_affinity_28</th>\n",
       "      <th>user-item_affinity_29</th>\n",
       "      <th>user-item_affinity_30</th>\n",
       "      <th>user-item_affinity_31</th>\n",
       "      <th>user-item_affinity_32</th>\n",
       "      <th>user-item_affinity_33</th>\n",
       "      <th>user-item_affinity_34</th>\n",
       "      <th>user-item_affinity_35</th>\n",
       "      <th>user-item_affinity_36</th>\n",
       "      <th>user-item_affinity_37</th>\n",
       "      <th>user-item_affinity_38</th>\n",
       "      <th>user-item_affinity_39</th>\n",
       "      <th>user-item_affinity_40</th>\n",
       "      <th>user-item_affinity_41</th>\n",
       "      <th>user-item_affinity_42</th>\n",
       "      <th>user-item_affinity_43</th>\n",
       "      <th>user-item_affinity_44</th>\n",
       "      <th>user-item_affinity_45</th>\n",
       "      <th>user-item_affinity_46</th>\n",
       "      <th>user-item_affinity_47</th>\n",
       "      <th>user-item_affinity_48</th>\n",
       "      <th>user-item_affinity_49</th>\n",
       "      <th>user-item_affinity_50</th>\n",
       "      <th>user-item_affinity_51</th>\n",
       "      <th>user-item_affinity_52</th>\n",
       "      <th>user-item_affinity_53</th>\n",
       "      <th>user-item_affinity_54</th>\n",
       "      <th>user-item_affinity_55</th>\n",
       "      <th>user-item_affinity_56</th>\n",
       "      <th>user-item_affinity_57</th>\n",
       "      <th>user-item_affinity_58</th>\n",
       "      <th>user-item_affinity_59</th>\n",
       "      <th>user-item_affinity_60</th>\n",
       "      <th>user-item_affinity_61</th>\n",
       "      <th>user-item_affinity_62</th>\n",
       "      <th>user-item_affinity_63</th>\n",
       "      <th>user-item_affinity_64</th>\n",
       "      <th>user-item_affinity_65</th>\n",
       "      <th>user-item_affinity_66</th>\n",
       "      <th>user-item_affinity_67</th>\n",
       "      <th>user-item_affinity_68</th>\n",
       "      <th>user-item_affinity_69</th>\n",
       "      <th>user-item_affinity_70</th>\n",
       "      <th>user-item_affinity_71</th>\n",
       "      <th>user-item_affinity_72</th>\n",
       "      <th>user-item_affinity_73</th>\n",
       "      <th>user-item_affinity_74</th>\n",
       "      <th>user-item_affinity_75</th>\n",
       "      <th>user-item_affinity_76</th>\n",
       "      <th>user-item_affinity_77</th>\n",
       "      <th>user-item_affinity_78</th>\n",
       "      <th>user-item_affinity_79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-24 00:00:17.004101+00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087125</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>A3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-24 00:00:19.715857+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>A0</td>\n",
       "      <td>B1</td>\n",
       "      <td>B2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-24 00:01:04.303227+00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>A3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-24 00:01:11.571162+00:00</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-24 00:02:41.811768+00:00</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  item_id  position  click  propensity_score user_feature_0 user_feature_1 user_feature_2 user_feature_3  user-item_affinity_0  user-item_affinity_1  user-item_affinity_2  user-item_affinity_3  user-item_affinity_4  user-item_affinity_5  user-item_affinity_6  user-item_affinity_7  user-item_affinity_8  user-item_affinity_9  user-item_affinity_10  user-item_affinity_11  user-item_affinity_12  user-item_affinity_13  user-item_affinity_14  user-item_affinity_15  user-item_affinity_16  user-item_affinity_17  user-item_affinity_18  user-item_affinity_19  user-item_affinity_20  user-item_affinity_21  user-item_affinity_22  user-item_affinity_23  user-item_affinity_24  user-item_affinity_25  user-item_affinity_26  user-item_affinity_27  user-item_affinity_28  user-item_affinity_29  user-item_affinity_30  user-item_affinity_31  user-item_affinity_32  user-item_affinity_33  user-item_affinity_34  user-item_affinity_35  user-item_affinity_36  \\\n",
       "0  2019-11-24 00:00:17.004101+00:00       79         2      0          0.087125             A0             A1             A2             A3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "1  2019-11-24 00:00:19.715857+00:00       14         1      0          0.006235             A0             B1             B2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "2  2019-11-24 00:01:04.303227+00:00       18         2      0          0.061300             A0             A1             C2             A3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "3  2019-11-24 00:01:11.571162+00:00       28         1      0          0.019430             A0             A1             A2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "4  2019-11-24 00:02:41.811768+00:00       65         2      0          0.019375             A0             A1             C2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "\n",
       "   user-item_affinity_37  user-item_affinity_38  user-item_affinity_39  user-item_affinity_40  user-item_affinity_41  user-item_affinity_42  user-item_affinity_43  user-item_affinity_44  user-item_affinity_45  user-item_affinity_46  user-item_affinity_47  user-item_affinity_48  user-item_affinity_49  user-item_affinity_50  user-item_affinity_51  user-item_affinity_52  user-item_affinity_53  user-item_affinity_54  user-item_affinity_55  user-item_affinity_56  user-item_affinity_57  user-item_affinity_58  user-item_affinity_59  user-item_affinity_60  user-item_affinity_61  user-item_affinity_62  user-item_affinity_63  user-item_affinity_64  user-item_affinity_65  user-item_affinity_66  user-item_affinity_67  user-item_affinity_68  user-item_affinity_69  user-item_affinity_70  user-item_affinity_71  user-item_affinity_72  user-item_affinity_73  user-item_affinity_74  user-item_affinity_75  user-item_affinity_76  user-item_affinity_77  user-item_affinity_78  user-item_affinity_79  \n",
       "0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "1                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "2                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "3                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "4                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9e363",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff4b35",
   "metadata": {},
   "source": [
    "### Counting user features vs user-item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8457a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3'], 80)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature_cols = [c for c in log_df_readable.columns if c.startswith('user_feature_')]\n",
    "affinity_cols = [c for c in log_df_readable.columns if c.startswith('user-item_affinity_')]\n",
    "\n",
    "user_feature_cols, len(affinity_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbd197dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>count</th>\n",
       "      <th>total_occurrences</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0</td>\n",
       "      <td>94</td>\n",
       "      <td>125</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_feature_0  count  total_occurrences  proportion\n",
       "0             A0     94                125       0.752\n",
       "1             B0     31                125       0.248"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Use the function for item_id=0 and user_feature_0\n",
    "temp_df_item_0 = compute_item_feature_distribution(log_df_readable, item_id=0, feature_col='user_feature_0')\n",
    "temp_df_item_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb5ce73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual propensity by item_id and user_feature_0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>count_of_occurrences</th>\n",
       "      <th>total</th>\n",
       "      <th>manual_propensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0</td>\n",
       "      <td>94</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>31</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A0</td>\n",
       "      <td>24</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>B0</td>\n",
       "      <td>7</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>A0</td>\n",
       "      <td>14</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>B0</td>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>A0</td>\n",
       "      <td>42</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>B0</td>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>A0</td>\n",
       "      <td>13</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>B0</td>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id user_feature_0  count_of_occurrences  total  manual_propensity\n",
       "0        0             A0                    94  10000             0.0094\n",
       "1        0             B0                    31  10000             0.0031\n",
       "2        1             A0                    24  10000             0.0024\n",
       "3        1             B0                     7  10000             0.0007\n",
       "4        2             A0                    14  10000             0.0014\n",
       "5        2             B0                     3  10000             0.0003\n",
       "6        3             A0                    42  10000             0.0042\n",
       "7        3             B0                     4  10000             0.0004\n",
       "8        4             A0                    13  10000             0.0013\n",
       "9        4             B0                     3  10000             0.0003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 187\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Compute manual propensity for item_id and user_feature_0\n",
    "manual_prop_uf0 = compute_manual_propensity(log_df_readable, categorical_col='user_feature_0')\n",
    "print(\"Manual propensity by item_id and user_feature_0:\")\n",
    "display(manual_prop_uf0.head(10))\n",
    "print(f\"\\nTotal rows: {len(manual_prop_uf0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1656feae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0</td>\n",
       "      <td>0.054190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>0.040818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id user_feature_0  propensity_score\n",
       "0        0             A0          0.054190\n",
       "1        0             B0          0.040818"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_0_manual_propensity_global_feature_0 =log_df_readable.groupby(['item_id', 'user_feature_0'])['propensity_score'].mean().reset_index()\n",
    "item_id_0_manual_propensity_global_feature_0 = item_id_0_manual_propensity_global_feature_0[item_id_0_manual_propensity_global_feature_0['item_id'] == 0]\n",
    "item_id_0_manual_propensity_global_feature_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e991ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05419"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable[(log_df_readable['item_id'] == 0) & (log_df_readable['user_feature_0'] == 'A0')]['propensity_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e124bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040818225806451607"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable[(log_df_readable['item_id'] == 0) & (log_df_readable['user_feature_0'] == 'B0')]['propensity_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83870f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable[(log_df_readable['item_id'] == 0)]['click'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "722f605f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024509803921568627"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable[(log_df_readable['item_id'] == 49)]['click'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89527b7",
   "metadata": {},
   "source": [
    "### Finding Missing User Feature Combinations for Item 0\n",
    "\n",
    "Let's identify which user feature combinations have NOT been observed for item_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a51e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values per feature:\n",
      "  user_feature_0: ['A0', 'B0', 'C0'] (n=3)\n",
      "  user_feature_1: ['A1', 'B1', 'C1', 'D1', 'E1'] (n=5)\n",
      "  user_feature_2: ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'I2'] (n=9)\n",
      "  user_feature_3: ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3', 'I3'] (n=9)\n",
      "\n",
      "Total possible feature combinations: 1,215\n"
     ]
    }
   ],
   "source": [
    "# Get all unique values for each user feature and calculate total combinations\n",
    "unique_values, total_combinations = compute_feature_combinations(\n",
    "    log_df_readable, \n",
    "    user_feature_cols, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "761a2a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed combinations for item_id=49: 95\n",
      "\n",
      "First 10 observed combinations:\n",
      "    user_feature_0 user_feature_1 user_feature_2 user_feature_3\n",
      "34              A0             A1             E2             A3\n",
      "70              A0             A1             E2             B3\n",
      "242             A0             A1             A2             B3\n",
      "516             A0             A1             C2             E3\n",
      "730             A0             B1             A2             E3\n",
      "819             A0             A1             B2             B3\n",
      "829             A0             A1             B2             C3\n",
      "858             A0             A1             C2             A3\n",
      "859             A0             A1             F2             B3\n",
      "860             A0             A1             B2             A3\n",
      "Missing combinations for item_id=49: 1,120\n",
      "Coverage: 7.82%\n",
      "\n",
      "First 10 missing combinations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>user_feature_1</th>\n",
       "      <th>user_feature_2</th>\n",
       "      <th>user_feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>C3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>F3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>G3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>H3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>I3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>C0</td>\n",
       "      <td>E1</td>\n",
       "      <td>I2</td>\n",
       "      <td>E3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>C0</td>\n",
       "      <td>E1</td>\n",
       "      <td>I2</td>\n",
       "      <td>F3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>C0</td>\n",
       "      <td>E1</td>\n",
       "      <td>I2</td>\n",
       "      <td>G3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>C0</td>\n",
       "      <td>E1</td>\n",
       "      <td>I2</td>\n",
       "      <td>H3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>C0</td>\n",
       "      <td>E1</td>\n",
       "      <td>I2</td>\n",
       "      <td>I3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1120 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_feature_0 user_feature_1 user_feature_2 user_feature_3\n",
       "2                A0             A1             A2             C3\n",
       "5                A0             A1             A2             F3\n",
       "6                A0             A1             A2             G3\n",
       "7                A0             A1             A2             H3\n",
       "8                A0             A1             A2             I3\n",
       "...             ...            ...            ...            ...\n",
       "1210             C0             E1             I2             E3\n",
       "1211             C0             E1             I2             F3\n",
       "1212             C0             E1             I2             G3\n",
       "1213             C0             E1             I2             H3\n",
       "1214             C0             E1             I2             I3\n",
       "\n",
       "[1120 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_missing_combinations(df, feature_cols, unique_values, item_id):\n",
    "    item_N_data = df[df['item_id'] == item_id]\n",
    "    observed_combos = item_N_data[feature_cols].drop_duplicates()\n",
    "\n",
    "    print(f\"Observed combinations for item_id={item_id}: {len(observed_combos)}\")\n",
    "    print(\"\\nFirst 10 observed combinations:\")\n",
    "    print(observed_combos.head(10))\n",
    "\n",
    "    # Create all possible combinations\n",
    "    all_possible = list(product(*[unique_values[col] for col in feature_cols]))\n",
    "    all_possible_df = pd.DataFrame(all_possible, columns=feature_cols)\n",
    "\n",
    "    # Find missing combinations (anti-join)\n",
    "    merged = all_possible_df.merge(\n",
    "        observed_combos,\n",
    "        on=feature_cols,\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    missing_combos = merged[merged['_merge'] == 'left_only'][feature_cols]\n",
    "\n",
    "    print(f\"Missing combinations for item_id={item_id}: {len(missing_combos):,}\")\n",
    "    print(f\"Coverage: {len(observed_combos)/len(all_possible_df)*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nFirst 10 missing combinations:\")\n",
    "    return missing_combos\n",
    "\n",
    "\n",
    "missing_combinations_item49 = get_missing_combinations(log_df_readable, user_feature_cols, unique_values, 49)\n",
    "missing_combinations_item49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a96e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A0'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_combinations_item49.iloc[0].user_feature_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a835d1",
   "metadata": {},
   "source": [
    "### Proving that this specific combination is indeed missing from the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f0a9edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>position</th>\n",
       "      <th>click</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>user_feature_1</th>\n",
       "      <th>user_feature_2</th>\n",
       "      <th>user_feature_3</th>\n",
       "      <th>user-item_affinity_0</th>\n",
       "      <th>user-item_affinity_1</th>\n",
       "      <th>user-item_affinity_2</th>\n",
       "      <th>user-item_affinity_3</th>\n",
       "      <th>user-item_affinity_4</th>\n",
       "      <th>user-item_affinity_5</th>\n",
       "      <th>user-item_affinity_6</th>\n",
       "      <th>user-item_affinity_7</th>\n",
       "      <th>user-item_affinity_8</th>\n",
       "      <th>user-item_affinity_9</th>\n",
       "      <th>user-item_affinity_10</th>\n",
       "      <th>user-item_affinity_11</th>\n",
       "      <th>user-item_affinity_12</th>\n",
       "      <th>user-item_affinity_13</th>\n",
       "      <th>user-item_affinity_14</th>\n",
       "      <th>user-item_affinity_15</th>\n",
       "      <th>user-item_affinity_16</th>\n",
       "      <th>user-item_affinity_17</th>\n",
       "      <th>user-item_affinity_18</th>\n",
       "      <th>user-item_affinity_19</th>\n",
       "      <th>user-item_affinity_20</th>\n",
       "      <th>user-item_affinity_21</th>\n",
       "      <th>user-item_affinity_22</th>\n",
       "      <th>user-item_affinity_23</th>\n",
       "      <th>user-item_affinity_24</th>\n",
       "      <th>user-item_affinity_25</th>\n",
       "      <th>user-item_affinity_26</th>\n",
       "      <th>user-item_affinity_27</th>\n",
       "      <th>user-item_affinity_28</th>\n",
       "      <th>user-item_affinity_29</th>\n",
       "      <th>user-item_affinity_30</th>\n",
       "      <th>user-item_affinity_31</th>\n",
       "      <th>user-item_affinity_32</th>\n",
       "      <th>user-item_affinity_33</th>\n",
       "      <th>user-item_affinity_34</th>\n",
       "      <th>user-item_affinity_35</th>\n",
       "      <th>user-item_affinity_36</th>\n",
       "      <th>user-item_affinity_37</th>\n",
       "      <th>user-item_affinity_38</th>\n",
       "      <th>user-item_affinity_39</th>\n",
       "      <th>user-item_affinity_40</th>\n",
       "      <th>user-item_affinity_41</th>\n",
       "      <th>user-item_affinity_42</th>\n",
       "      <th>user-item_affinity_43</th>\n",
       "      <th>user-item_affinity_44</th>\n",
       "      <th>user-item_affinity_45</th>\n",
       "      <th>user-item_affinity_46</th>\n",
       "      <th>user-item_affinity_47</th>\n",
       "      <th>user-item_affinity_48</th>\n",
       "      <th>user-item_affinity_49</th>\n",
       "      <th>user-item_affinity_50</th>\n",
       "      <th>user-item_affinity_51</th>\n",
       "      <th>user-item_affinity_52</th>\n",
       "      <th>user-item_affinity_53</th>\n",
       "      <th>user-item_affinity_54</th>\n",
       "      <th>user-item_affinity_55</th>\n",
       "      <th>user-item_affinity_56</th>\n",
       "      <th>user-item_affinity_57</th>\n",
       "      <th>user-item_affinity_58</th>\n",
       "      <th>user-item_affinity_59</th>\n",
       "      <th>user-item_affinity_60</th>\n",
       "      <th>user-item_affinity_61</th>\n",
       "      <th>user-item_affinity_62</th>\n",
       "      <th>user-item_affinity_63</th>\n",
       "      <th>user-item_affinity_64</th>\n",
       "      <th>user-item_affinity_65</th>\n",
       "      <th>user-item_affinity_66</th>\n",
       "      <th>user-item_affinity_67</th>\n",
       "      <th>user-item_affinity_68</th>\n",
       "      <th>user-item_affinity_69</th>\n",
       "      <th>user-item_affinity_70</th>\n",
       "      <th>user-item_affinity_71</th>\n",
       "      <th>user-item_affinity_72</th>\n",
       "      <th>user-item_affinity_73</th>\n",
       "      <th>user-item_affinity_74</th>\n",
       "      <th>user-item_affinity_75</th>\n",
       "      <th>user-item_affinity_76</th>\n",
       "      <th>user-item_affinity_77</th>\n",
       "      <th>user-item_affinity_78</th>\n",
       "      <th>user-item_affinity_79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, item_id, position, click, propensity_score, user_feature_0, user_feature_1, user_feature_2, user_feature_3, user-item_affinity_0, user-item_affinity_1, user-item_affinity_2, user-item_affinity_3, user-item_affinity_4, user-item_affinity_5, user-item_affinity_6, user-item_affinity_7, user-item_affinity_8, user-item_affinity_9, user-item_affinity_10, user-item_affinity_11, user-item_affinity_12, user-item_affinity_13, user-item_affinity_14, user-item_affinity_15, user-item_affinity_16, user-item_affinity_17, user-item_affinity_18, user-item_affinity_19, user-item_affinity_20, user-item_affinity_21, user-item_affinity_22, user-item_affinity_23, user-item_affinity_24, user-item_affinity_25, user-item_affinity_26, user-item_affinity_27, user-item_affinity_28, user-item_affinity_29, user-item_affinity_30, user-item_affinity_31, user-item_affinity_32, user-item_affinity_33, user-item_affinity_34, user-item_affinity_35, user-item_affinity_36, user-item_affinity_37, user-item_affinity_38, user-item_affinity_39, user-item_affinity_40, user-item_affinity_41, user-item_affinity_42, user-item_affinity_43, user-item_affinity_44, user-item_affinity_45, user-item_affinity_46, user-item_affinity_47, user-item_affinity_48, user-item_affinity_49, user-item_affinity_50, user-item_affinity_51, user-item_affinity_52, user-item_affinity_53, user-item_affinity_54, user-item_affinity_55, user-item_affinity_56, user-item_affinity_57, user-item_affinity_58, user-item_affinity_59, user-item_affinity_60, user-item_affinity_61, user-item_affinity_62, user-item_affinity_63, user-item_affinity_64, user-item_affinity_65, user-item_affinity_66, user-item_affinity_67, user-item_affinity_68, user-item_affinity_69, user-item_affinity_70, user-item_affinity_71, user-item_affinity_72, user-item_affinity_73, user-item_affinity_74, user-item_affinity_75, user-item_affinity_76, user-item_affinity_77, user-item_affinity_78, user-item_affinity_79]\n",
       "Index: []"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable_item49 = log_df_readable[log_df_readable['item_id'] == 49]\n",
    "log_df_readable_item49[(log_df_readable_item49['user_feature_0'] == missing_combinations_item49.iloc[0].user_feature_0) &\\\n",
    "                       (log_df_readable_item49['user_feature_1'] == missing_combinations_item49.iloc[0].user_feature_1) &\\\n",
    "                       (log_df_readable_item49['user_feature_2'] == missing_combinations_item49.iloc[0].user_feature_2) &\\\n",
    "                       (log_df_readable_item49['user_feature_3'] == missing_combinations_item49.iloc[0].user_feature_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f870468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>position</th>\n",
       "      <th>click</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>user_feature_0</th>\n",
       "      <th>user_feature_1</th>\n",
       "      <th>user_feature_2</th>\n",
       "      <th>user_feature_3</th>\n",
       "      <th>user-item_affinity_0</th>\n",
       "      <th>user-item_affinity_1</th>\n",
       "      <th>user-item_affinity_2</th>\n",
       "      <th>user-item_affinity_3</th>\n",
       "      <th>user-item_affinity_4</th>\n",
       "      <th>user-item_affinity_5</th>\n",
       "      <th>user-item_affinity_6</th>\n",
       "      <th>user-item_affinity_7</th>\n",
       "      <th>user-item_affinity_8</th>\n",
       "      <th>user-item_affinity_9</th>\n",
       "      <th>user-item_affinity_10</th>\n",
       "      <th>user-item_affinity_11</th>\n",
       "      <th>user-item_affinity_12</th>\n",
       "      <th>user-item_affinity_13</th>\n",
       "      <th>user-item_affinity_14</th>\n",
       "      <th>user-item_affinity_15</th>\n",
       "      <th>user-item_affinity_16</th>\n",
       "      <th>user-item_affinity_17</th>\n",
       "      <th>user-item_affinity_18</th>\n",
       "      <th>user-item_affinity_19</th>\n",
       "      <th>user-item_affinity_20</th>\n",
       "      <th>user-item_affinity_21</th>\n",
       "      <th>user-item_affinity_22</th>\n",
       "      <th>user-item_affinity_23</th>\n",
       "      <th>user-item_affinity_24</th>\n",
       "      <th>user-item_affinity_25</th>\n",
       "      <th>user-item_affinity_26</th>\n",
       "      <th>user-item_affinity_27</th>\n",
       "      <th>user-item_affinity_28</th>\n",
       "      <th>user-item_affinity_29</th>\n",
       "      <th>user-item_affinity_30</th>\n",
       "      <th>user-item_affinity_31</th>\n",
       "      <th>user-item_affinity_32</th>\n",
       "      <th>user-item_affinity_33</th>\n",
       "      <th>user-item_affinity_34</th>\n",
       "      <th>user-item_affinity_35</th>\n",
       "      <th>user-item_affinity_36</th>\n",
       "      <th>user-item_affinity_37</th>\n",
       "      <th>user-item_affinity_38</th>\n",
       "      <th>user-item_affinity_39</th>\n",
       "      <th>user-item_affinity_40</th>\n",
       "      <th>user-item_affinity_41</th>\n",
       "      <th>user-item_affinity_42</th>\n",
       "      <th>user-item_affinity_43</th>\n",
       "      <th>user-item_affinity_44</th>\n",
       "      <th>user-item_affinity_45</th>\n",
       "      <th>user-item_affinity_46</th>\n",
       "      <th>user-item_affinity_47</th>\n",
       "      <th>user-item_affinity_48</th>\n",
       "      <th>user-item_affinity_49</th>\n",
       "      <th>user-item_affinity_50</th>\n",
       "      <th>user-item_affinity_51</th>\n",
       "      <th>user-item_affinity_52</th>\n",
       "      <th>user-item_affinity_53</th>\n",
       "      <th>user-item_affinity_54</th>\n",
       "      <th>user-item_affinity_55</th>\n",
       "      <th>user-item_affinity_56</th>\n",
       "      <th>user-item_affinity_57</th>\n",
       "      <th>user-item_affinity_58</th>\n",
       "      <th>user-item_affinity_59</th>\n",
       "      <th>user-item_affinity_60</th>\n",
       "      <th>user-item_affinity_61</th>\n",
       "      <th>user-item_affinity_62</th>\n",
       "      <th>user-item_affinity_63</th>\n",
       "      <th>user-item_affinity_64</th>\n",
       "      <th>user-item_affinity_65</th>\n",
       "      <th>user-item_affinity_66</th>\n",
       "      <th>user-item_affinity_67</th>\n",
       "      <th>user-item_affinity_68</th>\n",
       "      <th>user-item_affinity_69</th>\n",
       "      <th>user-item_affinity_70</th>\n",
       "      <th>user-item_affinity_71</th>\n",
       "      <th>user-item_affinity_72</th>\n",
       "      <th>user-item_affinity_73</th>\n",
       "      <th>user-item_affinity_74</th>\n",
       "      <th>user-item_affinity_75</th>\n",
       "      <th>user-item_affinity_76</th>\n",
       "      <th>user-item_affinity_77</th>\n",
       "      <th>user-item_affinity_78</th>\n",
       "      <th>user-item_affinity_79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-24 00:00:17.004101+00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087125</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>A3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-24 00:00:19.715857+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>A0</td>\n",
       "      <td>B1</td>\n",
       "      <td>B2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-24 00:01:04.303227+00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>A3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-24 00:01:11.571162+00:00</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-24 00:02:41.811768+00:00</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>A0</td>\n",
       "      <td>A1</td>\n",
       "      <td>C2</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  item_id  position  click  propensity_score user_feature_0 user_feature_1 user_feature_2 user_feature_3  user-item_affinity_0  user-item_affinity_1  user-item_affinity_2  user-item_affinity_3  user-item_affinity_4  user-item_affinity_5  user-item_affinity_6  user-item_affinity_7  user-item_affinity_8  user-item_affinity_9  user-item_affinity_10  user-item_affinity_11  user-item_affinity_12  user-item_affinity_13  user-item_affinity_14  user-item_affinity_15  user-item_affinity_16  user-item_affinity_17  user-item_affinity_18  user-item_affinity_19  user-item_affinity_20  user-item_affinity_21  user-item_affinity_22  user-item_affinity_23  user-item_affinity_24  user-item_affinity_25  user-item_affinity_26  user-item_affinity_27  user-item_affinity_28  user-item_affinity_29  user-item_affinity_30  user-item_affinity_31  user-item_affinity_32  user-item_affinity_33  user-item_affinity_34  user-item_affinity_35  user-item_affinity_36  \\\n",
       "0  2019-11-24 00:00:17.004101+00:00       79         2      0          0.087125             A0             A1             A2             A3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "1  2019-11-24 00:00:19.715857+00:00       14         1      0          0.006235             A0             B1             B2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "2  2019-11-24 00:01:04.303227+00:00       18         2      0          0.061300             A0             A1             C2             A3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "3  2019-11-24 00:01:11.571162+00:00       28         1      0          0.019430             A0             A1             A2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "4  2019-11-24 00:02:41.811768+00:00       65         2      0          0.019375             A0             A1             C2             B3                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0   \n",
       "\n",
       "   user-item_affinity_37  user-item_affinity_38  user-item_affinity_39  user-item_affinity_40  user-item_affinity_41  user-item_affinity_42  user-item_affinity_43  user-item_affinity_44  user-item_affinity_45  user-item_affinity_46  user-item_affinity_47  user-item_affinity_48  user-item_affinity_49  user-item_affinity_50  user-item_affinity_51  user-item_affinity_52  user-item_affinity_53  user-item_affinity_54  user-item_affinity_55  user-item_affinity_56  user-item_affinity_57  user-item_affinity_58  user-item_affinity_59  user-item_affinity_60  user-item_affinity_61  user-item_affinity_62  user-item_affinity_63  user-item_affinity_64  user-item_affinity_65  user-item_affinity_66  user-item_affinity_67  user-item_affinity_68  user-item_affinity_69  user-item_affinity_70  user-item_affinity_71  user-item_affinity_72  user-item_affinity_73  user-item_affinity_74  user-item_affinity_75  user-item_affinity_76  user-item_affinity_77  user-item_affinity_78  user-item_affinity_79  \n",
       "0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "1                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "2                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "3                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  \n",
       "4                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0                    0.0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df_readable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e16924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST MODEL FOR CLICK PREDICTION\n",
      "============================================================\n",
      "Total rows in dataset: 10,000\n",
      "Features: ['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3', 'item_id']\n",
      "Target: click\n",
      "Target distribution:\n",
      "0    9958\n",
      "1      42\n",
      "Name: click, dtype: int64\n",
      "Click rate: 0.0042\n",
      "\n",
      "========================================\n",
      "ENCODING CATEGORICAL FEATURES\n",
      "========================================\n",
      "user_feature_0: 3 unique values\n",
      "  Sample mapping: {'A0': 0, 'B0': 1, 'C0': 2}\n",
      "user_feature_1: 5 unique values\n",
      "  Sample mapping: {'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4}\n",
      "user_feature_2: 9 unique values\n",
      "  Sample mapping: {'A2': 0, 'B2': 1, 'C2': 2, 'D2': 3, 'E2': 4}\n",
      "user_feature_3: 9 unique values\n",
      "  Sample mapping: {'A3': 0, 'B3': 1, 'C3': 2, 'D3': 3, 'E3': 4}\n",
      "\n",
      "Final feature matrix shape: (10000, 5)\n",
      "Target vector shape: (10000,)\n",
      "\n",
      "========================================\n",
      "TRAIN-TEST SPLIT\n",
      "========================================\n",
      "Training set: 8,000 samples\n",
      "Test set: 2,000 samples\n",
      "Train click rate: 0.0043\n",
      "Test click rate: 0.0040\n",
      "\n",
      "========================================\n",
      "TRAINING RANDOM FOREST\n",
      "========================================\n",
      "\n",
      "Model Performance:\n",
      "Train Accuracy: 0.9958\n",
      "Test Accuracy: 0.9960\n",
      "Test AUC: 0.3851\n",
      "\n",
      "Feature Importances:\n",
      "  user_feature_0 : 0.0340\n",
      "  user_feature_1 : 0.0270\n",
      "  user_feature_2 : 0.2267\n",
      "  user_feature_3 : 0.1940\n",
      "  item_id        : 0.5183\n",
      "\n",
      "============================================================\n",
      "PREDICTION FOR TARGET COMBINATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model to Predict Click Probability\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST MODEL FOR CLICK PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Prepare the features and target\n",
    "feature_cols = ['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3', 'item_id']\n",
    "target_col = 'click'\n",
    "\n",
    "# Check data availability\n",
    "print(f\"Total rows in dataset: {len(log_df_readable):,}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(log_df_readable[target_col].value_counts())\n",
    "print(f\"Click rate: {log_df_readable[target_col].mean():.4f}\")\n",
    "\n",
    "# 2. Encode categorical features\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create label encoders for categorical features\n",
    "label_encoders = {}\n",
    "X_encoded = log_df_readable[feature_cols].copy()\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col != 'item_id':  # item_id is already numeric\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(log_df_readable[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"{col}: {len(le.classes_)} unique values\")\n",
    "        print(f\"  Sample mapping: {dict(zip(le.classes_[:5], le.transform(le.classes_[:5])))}\")\n",
    "\n",
    "y = log_df_readable[target_col]\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X_encoded.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# 3. Train-test split\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Train click rate: {y_train.mean():.4f}\")\n",
    "print(f\"Test click rate: {y_test.mean():.4f}\")\n",
    "\n",
    "# 4. Train Random Forest\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Model evaluation\n",
    "print(\"\\nModel Performance:\")\n",
    "train_score = rf_model.score(X_train, y_train)\n",
    "test_score = rf_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score:.4f}\")\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Predictions for AUC\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\nFeature Importances:\")\n",
    "for col, importance in zip(feature_cols, rf_model.feature_importances_):\n",
    "    print(f\"  {col:15s}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION FOR TARGET COMBINATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d821064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target combination to predict:\n",
      "  user_feature_0: A0\n",
      "  user_feature_1: A1\n",
      "  user_feature_2: A2\n",
      "  user_feature_3: C3\n",
      "  item_id: 49\n",
      "\n",
      "Combination exists in data: 0 observations\n",
      "This is an UNSEEN combination - model will extrapolate\n",
      "\n",
      "Encoded values:\n",
      "  user_feature_0: A0 -> 0\n",
      "  user_feature_1: A1 -> 0\n",
      "  user_feature_2: A2 -> 0\n",
      "  user_feature_3: C3 -> 2\n",
      "  item_id: 49 -> 49\n",
      "\n",
      "Prediction input shape: (1, 5)\n",
      "Prediction input:\n",
      "   user_feature_0  user_feature_1  user_feature_2  user_feature_3  item_id\n",
      "0               0               0               0               2       49\n",
      "\n",
      "==================================================\n",
      "FINAL PREDICTION RESULTS\n",
      "==================================================\n",
      "Predicted click probability: 0.000970\n",
      "Predicted class (click): 0\n",
      "Confidence: 0.999030\n",
      "\n",
      "Comparison with baselines:\n",
      "Overall CTR: 0.004200\n",
      "Item 49 CTR: 0.002451\n",
      "Predicted CTR: 0.000970\n",
      "\n",
      "Model confidence analysis:\n",
      " Model predicts LOWER than item 49 average (-0.15%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Predict for the specific combination\n",
    "target_combination = {\n",
    "    'user_feature_0': 'A0',\n",
    "    'user_feature_1': 'A1', \n",
    "    'user_feature_2': 'A2',\n",
    "    'user_feature_3': 'C3',\n",
    "    'item_id': 49\n",
    "}\n",
    "\n",
    "print(f\"Target combination to predict:\")\n",
    "for feature, value in target_combination.items():\n",
    "    print(f\"  {feature}: {value}\")\n",
    "\n",
    "# Check if this combination exists in the training data\n",
    "exists_in_data = log_df_readable[\n",
    "    (log_df_readable['user_feature_0'] == target_combination['user_feature_0']) &\n",
    "    (log_df_readable['user_feature_1'] == target_combination['user_feature_1']) &\n",
    "    (log_df_readable['user_feature_2'] == target_combination['user_feature_2']) &\n",
    "    (log_df_readable['user_feature_3'] == target_combination['user_feature_3']) &\n",
    "    (log_df_readable['item_id'] == target_combination['item_id'])\n",
    "]\n",
    "\n",
    "print(f\"\\nCombination exists in data: {len(exists_in_data)} observations\")\n",
    "if len(exists_in_data) > 0:\n",
    "    observed_ctr = exists_in_data['click'].mean()\n",
    "    print(f\"Observed CTR: {observed_ctr:.4f}\")\n",
    "else:\n",
    "    print(\"This is an UNSEEN combination - model will extrapolate\")\n",
    "\n",
    "# Encode the target combination\n",
    "target_encoded = {}\n",
    "for feature, value in target_combination.items():\n",
    "    if feature in label_encoders:\n",
    "        try:\n",
    "            target_encoded[feature] = label_encoders[feature].transform([str(value)])[0]\n",
    "        except ValueError:\n",
    "            print(f\"Warning: {value} not seen in training for {feature}\")\n",
    "            # Use the most common value as fallback\n",
    "            target_encoded[feature] = 0\n",
    "    else:\n",
    "        # item_id is numeric\n",
    "        target_encoded[feature] = value\n",
    "\n",
    "print(f\"\\nEncoded values:\")\n",
    "for feature, encoded_val in target_encoded.items():\n",
    "    print(f\"  {feature}: {target_combination[feature]} -> {encoded_val}\")\n",
    "\n",
    "# Create prediction input\n",
    "X_target = pd.DataFrame([target_encoded])\n",
    "print(f\"\\nPrediction input shape: {X_target.shape}\")\n",
    "print(f\"Prediction input:\\n{X_target}\")\n",
    "\n",
    "# Make prediction\n",
    "click_probability = rf_model.predict_proba(X_target)[0, 1]\n",
    "click_prediction = rf_model.predict(X_target)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL PREDICTION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Predicted click probability: {click_probability:.6f}\")\n",
    "print(f\"Predicted class (click): {click_prediction}\")\n",
    "print(f\"Confidence: {max(rf_model.predict_proba(X_target)[0]):.6f}\")\n",
    "\n",
    "# Compare with baseline rates\n",
    "overall_ctr = log_df_readable['click'].mean()\n",
    "item_49_ctr = log_df_readable[log_df_readable['item_id'] == 49]['click'].mean()\n",
    "\n",
    "print(f\"\\nComparison with baselines:\")\n",
    "print(f\"Overall CTR: {overall_ctr:.6f}\")\n",
    "print(f\"Item 49 CTR: {item_49_ctr:.6f}\")\n",
    "print(f\"Predicted CTR: {click_probability:.6f}\")\n",
    "\n",
    "print(f\"\\nModel confidence analysis:\")\n",
    "if click_probability > item_49_ctr:\n",
    "    print(f\" Model predicts HIGHER than item 49 average (+{(click_probability-item_49_ctr)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\" Model predicts LOWER than item 49 average ({(click_probability-item_49_ctr)*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20bd5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDITIONAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. PARTIAL FEATURE ANALYSIS:\n",
      "------------------------------\n",
      "user_feature_0=A0: 0.002950 CTR (n=339)\n",
      "user_feature_1=A1: 0.002890 CTR (n=346)\n",
      "user_feature_2=A2: 0.015625 CTR (n=64)\n",
      "user_feature_3=C3: 0.000000 CTR (n=24)\n",
      "\n",
      "Overall item 49 CTR: 0.002451 (n=408)\n",
      "\n",
      "2. PREDICTION CONFIDENCE ANALYSIS:\n",
      "-----------------------------------\n",
      "Prediction mean: 0.000970\n",
      "Prediction std: 0.005122\n",
      "90% Confidence interval: [0.000000, 0.000089]\n",
      "Prediction range: 0.000089\n",
      "\n",
      "3. MODEL INTERPRETATION:\n",
      "-------------------------\n",
      " This combination was NOT observed in the training data\n",
      " Model extrapolates based on similar feature patterns\n",
      " Low prediction (0.000970) suggests this combination is unlikely to generate clicks\n",
      " Prediction is below both overall CTR and item 49 CTR\n",
      " High confidence (99.9%) in negative prediction\n",
      " Prediction uncertainty: HIGH (std: 0.005122)\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/armandoordoricadelatorre/miniconda/envs/obp_replication/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Additional Analysis: Feature Impact and Confidence Intervals\n",
    "print(\"=\"*60)\n",
    "print(\"ADDITIONAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check individual feature contributions by looking at similar combinations\n",
    "print(\"\\n1. PARTIAL FEATURE ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Look at item 49 performance with individual feature matches\n",
    "item_49_data = log_df_readable[log_df_readable['item_id'] == 49]\n",
    "\n",
    "for feature, value in target_combination.items():\n",
    "    if feature != 'item_id':\n",
    "        matching_data = item_49_data[item_49_data[feature] == value]\n",
    "        if len(matching_data) > 0:\n",
    "            ctr = matching_data['click'].mean()\n",
    "            print(f\"{feature}={value}: {ctr:.6f} CTR (n={len(matching_data)})\")\n",
    "        else:\n",
    "            print(f\"{feature}={value}: No data available\")\n",
    "\n",
    "print(f\"\\nOverall item 49 CTR: {item_49_data['click'].mean():.6f} (n={len(item_49_data)})\")\n",
    "\n",
    "# Model prediction intervals (using tree predictions)\n",
    "print(\"\\n2. PREDICTION CONFIDENCE ANALYSIS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Get predictions from individual trees for uncertainty estimation\n",
    "tree_predictions = []\n",
    "for estimator in rf_model.estimators_:\n",
    "    pred = estimator.predict_proba(X_target)[0, 1]\n",
    "    tree_predictions.append(pred)\n",
    "\n",
    "tree_predictions = np.array(tree_predictions)\n",
    "pred_mean = tree_predictions.mean()\n",
    "pred_std = tree_predictions.std()\n",
    "pred_ci_lower = np.percentile(tree_predictions, 5)\n",
    "pred_ci_upper = np.percentile(tree_predictions, 95)\n",
    "\n",
    "print(f\"Prediction mean: {pred_mean:.6f}\")\n",
    "print(f\"Prediction std: {pred_std:.6f}\")\n",
    "print(f\"90% Confidence interval: [{pred_ci_lower:.6f}, {pred_ci_upper:.6f}]\")\n",
    "print(f\"Prediction range: {pred_ci_upper - pred_ci_lower:.6f}\")\n",
    "\n",
    "# Model interpretation\n",
    "print(\"\\n3. MODEL INTERPRETATION:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\" This combination was NOT observed in the training data\")\n",
    "print(f\" Model extrapolates based on similar feature patterns\")\n",
    "print(f\" Low prediction ({click_probability:.6f}) suggests this combination is unlikely to generate clicks\")\n",
    "print(f\" Prediction is below both overall CTR and item 49 CTR\")\n",
    "print(f\" High confidence (99.9%) in negative prediction\")\n",
    "\n",
    "# Risk assessment\n",
    "risk_level = \"LOW\" if pred_std < 0.001 else \"MEDIUM\" if pred_std < 0.005 else \"HIGH\"\n",
    "print(f\" Prediction uncertainty: {risk_level} (std: {pred_std:.6f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a403a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cad81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde0a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67612303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1671439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "def bayes_chain_ctr(feature_tables: dict,\n",
    "                    chosen_values: dict,\n",
    "                    alpha_click=1.0, beta_click=1.0,\n",
    "                    alpha_cond=1.0):\n",
    "    \"\"\"\n",
    "    Estimate P(click | f0, f1, ..., fk) for a single item using a Nave Bayes (Bayesian chain rule)\n",
    "    on per-feature aggregates with Laplace smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_tables : dict[str, pd.DataFrame]\n",
    "        For each feature name (e.g., 'user_feature_0'), a DataFrame with columns:\n",
    "        ['feature_value','clicks','no_clicks','total'] covering *one item* and partitioned by that feature.\n",
    "    chosen_values : dict[str, str]\n",
    "        Mapping feature name -> chosen category value (e.g., {'user_feature_0':'A0', ...}).\n",
    "    alpha_click, beta_click : float\n",
    "        Beta prior on global CTR (for baseline odds). Defaults to Beta(1,1) Laplace.\n",
    "    alpha_cond : float\n",
    "        Laplace smoothing added to each category when computing P(value | click) and P(value | no click).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "      'p_baseline' : baseline CTR for the item (smoothed),\n",
    "      'odds_baseline' : baseline odds,\n",
    "      'lr_by_feature' : dict of likelihood ratios per feature,\n",
    "      'odds_posterior' : combined odds,\n",
    "      'p_posterior' : combined CTR estimate\n",
    "    \"\"\"\n",
    "    # Use the first table to get global totals (all tables partition the same item)\n",
    "    first = next(iter(feature_tables.values()))\n",
    "    total_clicks = int(first['clicks'].sum())\n",
    "    total_noclk = int(first['no_clicks'].sum())\n",
    "    total_impr = total_clicks + total_noclk\n",
    "\n",
    "    # 1) Baseline CTR with Beta(alpha_click, beta_click)\n",
    "    p0 = (total_clicks + alpha_click) / (total_impr + alpha_click + beta_click)\n",
    "    odds0 = p0 / (1.0 - p0)\n",
    "\n",
    "    # 2) Per-feature likelihood ratios: P(f=value | click) / P(f=value | no-click)\n",
    "    lr = {}\n",
    "    for feat, df in feature_tables.items():\n",
    "        # how many categories for smoothing denominator\n",
    "        K = df['feature_value'].nunique()\n",
    "\n",
    "        row = df.loc[df['feature_value'] == chosen_values[feat]]\n",
    "        if row.empty:\n",
    "            # unseen category for this feature: treat as 0 counts (will be handled by smoothing)\n",
    "            c_clicks = 0\n",
    "            c_noclk = 0\n",
    "        else:\n",
    "            c_clicks = int(row['clicks'].iloc[0])\n",
    "            c_noclk = int(row['no_clicks'].iloc[0])\n",
    "\n",
    "        # P(value | click)\n",
    "        p_val_given_click = (c_clicks + alpha_cond) / (total_clicks + alpha_cond * K)\n",
    "        # P(value | no-click)\n",
    "        p_val_given_noclk = (c_noclk + alpha_cond) / (total_noclk + alpha_cond * K)\n",
    "\n",
    "        # likelihood ratio; tiny floor to avoid division by ~0\n",
    "        lr[feat] = p_val_given_click / max(p_val_given_noclk, 1e-15)\n",
    "\n",
    "    # 3) Combine odds\n",
    "    odds_post = odds0 * reduce(mul, lr.values(), 1.0)\n",
    "    p_post = odds_post / (1.0 + odds_post)\n",
    "\n",
    "    return {\n",
    "        'p_baseline': p0,\n",
    "        'odds_baseline': odds0,\n",
    "        'lr_by_feature': lr,\n",
    "        'odds_posterior': odds_post,\n",
    "        'p_posterior': p_post\n",
    "    }\n",
    "\n",
    "\n",
    "def get_empirical_CTR_by_category(input_df, item_id, category_col):\n",
    "    item_df = input_df[input_df['item_id'] == item_id]\n",
    "    click_stats = item_df.groupby([category_col]).agg({\n",
    "        'click': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    click_stats.columns = [category_col, 'clicks', 'total', 'click_rate']\n",
    "\n",
    "    # Add no-clicks column\n",
    "    click_stats['no_clicks'] = click_stats['total'] - click_stats['clicks']\n",
    "\n",
    "    # Reorder columns for clarity\n",
    "    click_stats = click_stats[[category_col, 'clicks', 'no_clicks', 'total', 'click_rate']]\n",
    "    click_stats['item_id'] = item_id\n",
    "\n",
    "    return click_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e900cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed click statistics for item_id=49 by user_feature_0:\n",
      "============================================================\n",
      "  user_feature_0  clicks  no_clicks  total  click_rate  item_id\n",
      "0             A0       1        338    339     0.00295       49\n",
      "1             B0       0         63     63     0.00000       49\n",
      "2             C0       0          6      6     0.00000       49\n",
      "\n",
      "Detailed click statistics for item_id=49 by user_feature_1:\n",
      "============================================================\n",
      "  user_feature_1  clicks  no_clicks  total  click_rate  item_id\n",
      "0             A1       1        345    346     0.00289       49\n",
      "1             B1       0         36     36     0.00000       49\n",
      "2             C1       0          8      8     0.00000       49\n",
      "3             D1       0         18     18     0.00000       49\n",
      "\n",
      "Detailed click statistics for item_id=49 by user_feature_2:\n",
      "============================================================\n",
      "  user_feature_2  clicks  no_clicks  total  click_rate  item_id\n",
      "0             A2       1         63     64    0.015625       49\n",
      "1             B2       0         94     94    0.000000       49\n",
      "2             C2       0         92     92    0.000000       49\n",
      "3             D2       0         60     60    0.000000       49\n",
      "4             E2       0         69     69    0.000000       49\n",
      "5             F2       0         21     21    0.000000       49\n",
      "6             G2       0          8      8    0.000000       49\n",
      "\n",
      "Detailed click statistics for item_id=49 by user_feature_3:\n",
      "============================================================\n",
      "  user_feature_3  clicks  no_clicks  total  click_rate  item_id\n",
      "0             A3       1        154    155    0.006452       49\n",
      "1             B3       0        146    146    0.000000       49\n",
      "2             C3       0         24     24    0.000000       49\n",
      "3             D3       0         45     45    0.000000       49\n",
      "4             E3       0         31     31    0.000000       49\n",
      "5             F3       0          5      5    0.000000       49\n",
      "6             H3       0          2      2    0.000000       49\n"
     ]
    }
   ],
   "source": [
    "# Detailed click statistics by user_feature_0\n",
    "N=49\n",
    "\n",
    "user_features_list = ['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3']\n",
    "\n",
    "for feature in user_features_list:\n",
    "    click_stats = get_empirical_CTR_by_category(log_df_readable, N, feature)\n",
    "    print(f\"\\nDetailed click statistics for item_id={N} by {feature}:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(click_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2dbb9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature table for user_feature_0:\n",
      "  feature_value  clicks  no_clicks  total\n",
      "0            A0       1        338    339\n",
      "1            B0       0         63     63\n",
      "2            C0       0          6      6\n",
      "\n",
      "Feature table for user_feature_1:\n",
      "  feature_value  clicks  no_clicks  total\n",
      "0            A1       1        345    346\n",
      "1            B1       0         36     36\n",
      "2            C1       0          8      8\n",
      "3            D1       0         18     18\n",
      "\n",
      "Feature table for user_feature_2:\n",
      "  feature_value  clicks  no_clicks  total\n",
      "0            A2       1         63     64\n",
      "1            B2       0         94     94\n",
      "2            C2       0         92     92\n",
      "3            D2       0         60     60\n",
      "4            E2       0         69     69\n",
      "5            F2       0         21     21\n",
      "6            G2       0          8      8\n",
      "\n",
      "Feature table for user_feature_3:\n",
      "  feature_value  clicks  no_clicks  total\n",
      "0            A3       1        154    155\n",
      "1            B3       0        146    146\n",
      "2            C3       0         24     24\n",
      "3            D3       0         45     45\n",
      "4            E3       0         31     31\n",
      "5            F3       0          5      5\n",
      "6            H3       0          2      2\n",
      "\n",
      "Chosen combination: {'user_feature_0': 'A0', 'user_feature_1': 'A1', 'user_feature_2': 'E2', 'user_feature_3': 'A3'}\n",
      "\n",
      "============================================================\n",
      "BAYESIAN CHAIN RULE RESULTS:\n",
      "============================================================\n",
      "Baseline CTR (smoothed): 0.004878\n",
      "Likelihood ratios by feature:\n",
      "  user_feature_0: 0.604720\n",
      "  user_feature_1: 0.475145\n",
      "  user_feature_2: 0.739286\n",
      "  user_feature_3: 0.667742\n",
      "Posterior CTR estimate: 0.000695\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Build per-feature tables dynamically from the empirical data\n",
    "N = 49  # item_id we're analyzing\n",
    "user_features_list = ['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3']\n",
    "\n",
    "# Extract data for each feature using our function\n",
    "feature_tables = {}\n",
    "for feature in user_features_list:\n",
    "    click_stats = get_empirical_CTR_by_category(log_df_readable, N, feature)\n",
    "    \n",
    "    # Convert to the format needed for bayes_chain_ctr\n",
    "    feature_df = pd.DataFrame({\n",
    "        'feature_value': click_stats[feature].values,\n",
    "        'clicks': click_stats['clicks'].values,\n",
    "        'no_clicks': click_stats['no_clicks'].values,\n",
    "        'total': click_stats['total'].values,\n",
    "    })\n",
    "    \n",
    "    feature_tables[feature] = feature_df\n",
    "    \n",
    "    print(f\"\\nFeature table for {feature}:\")\n",
    "    print(feature_df)\n",
    "\n",
    "# Define the combination we want to estimate CTR for\n",
    "chosen = {\n",
    "    'user_feature_0': 'A0',\n",
    "    'user_feature_1': 'A1', \n",
    "    'user_feature_2': 'E2',  # Changed from A2 to E2 as per your target\n",
    "    'user_feature_3': 'A3',  # Changed from C3 to A3 as per your target\n",
    "}\n",
    "\n",
    "print(f\"\\nChosen combination: {chosen}\")\n",
    "\n",
    "# Apply Bayesian chain rule estimation\n",
    "res = bayes_chain_ctr(feature_tables, chosen,\n",
    "                      alpha_click=1.0, beta_click=1.0,  # Beta prior on global CTR\n",
    "                      alpha_cond=1.0)                   # Laplace per-category\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN CHAIN RULE RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline CTR (smoothed): {res['p_baseline']:.6f}\")\n",
    "print(f\"Likelihood ratios by feature:\")\n",
    "for feat, lr in res['lr_by_feature'].items():\n",
    "    print(f\"  {feat}: {lr:.6f}\")\n",
    "print(f\"Posterior CTR estimate: {res['p_posterior']:.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2b490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2966c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target: item 49 with user features (A0, A1, E2, A3)\n",
    "target_item_49 = 49\n",
    "target_features_49 = ['A0', 'A1', 'E2', 'A3']\n",
    "\n",
    "print(f\"Target: item_id={target_item_49}, user_features={target_features_49}\")\n",
    "\n",
    "# Check if this combination exists in the dataset\n",
    "exists_49 = log_df_readable[\n",
    "    (log_df_readable['item_id'] == target_item_49) &\n",
    "    (log_df_readable['user_feature_0'] == target_features_49[0]) &\n",
    "    (log_df_readable['user_feature_1'] == target_features_49[1]) &\n",
    "    (log_df_readable['user_feature_2'] == target_features_49[2]) &\n",
    "    (log_df_readable['user_feature_3'] == target_features_49[3])\n",
    "]\n",
    "\n",
    "print(f\"\\nObservations with this exact combination: {len(exists_49)}\")\n",
    "if len(exists_49) > 0:\n",
    "    print(f\"Observed CTR: {exists_49['click'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"This is an UNOBSERVED combination - we need to estimate CTR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48eaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 1: Marginal Averaging for Item 49\n",
    "\n",
    "# Filter to only item 49\n",
    "item_49_data = log_df_readable[log_df_readable['item_id'] == target_item_49].copy()\n",
    "\n",
    "print(f\"Total observations for item {target_item_49}: {len(item_49_data)}\")\n",
    "print(f\"Overall CTR for item {target_item_49}: {item_49_data['click'].mean():.4f}\")\n",
    "\n",
    "# Calculate CTR for different levels of feature matching\n",
    "marginal_estimates_49 = {}\n",
    "\n",
    "# Overall item CTR (no feature matching)\n",
    "marginal_estimates_49['overall_item'] = item_49_data['click'].mean()\n",
    "\n",
    "# Match each individual feature\n",
    "for i, feat_val in enumerate(target_features_49):\n",
    "    feat_name = f'user_feature_{i}'\n",
    "    matching_data = item_49_data[item_49_data[feat_name] == feat_val]\n",
    "    if len(matching_data) > 0:\n",
    "        marginal_estimates_49[f'{feat_name}={feat_val}'] = matching_data['click'].mean()\n",
    "        print(f\"CTR when {feat_name}={feat_val}: {marginal_estimates_49[f'{feat_name}={feat_val}']:.4f} (n={len(matching_data)})\")\n",
    "    else:\n",
    "        marginal_estimates_49[f'{feat_name}={feat_val}'] = np.nan\n",
    "        print(f\"CTR when {feat_name}={feat_val}: NO DATA\")\n",
    "\n",
    "# Match pairs of features\n",
    "feature_pairs_49 = [\n",
    "    (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)\n",
    "]\n",
    "\n",
    "for i, j in feature_pairs_49:\n",
    "    feat_i, feat_j = f'user_feature_{i}', f'user_feature_{j}'\n",
    "    val_i, val_j = target_features_49[i], target_features_49[j]\n",
    "    matching_data = item_49_data[\n",
    "        (item_49_data[feat_i] == val_i) & \n",
    "        (item_49_data[feat_j] == val_j)\n",
    "    ]\n",
    "    if len(matching_data) > 0:\n",
    "        key = f'{feat_i}={val_i}, {feat_j}={val_j}'\n",
    "        marginal_estimates_49[key] = matching_data['click'].mean()\n",
    "        print(f\"CTR when {key}: {marginal_estimates_49[key]:.4f} (n={len(matching_data)})\")\n",
    "\n",
    "# Match triples\n",
    "feature_triples_49 = [\n",
    "    (0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)\n",
    "]\n",
    "\n",
    "for i, j, k in feature_triples_49:\n",
    "    feat_i, feat_j, feat_k = f'user_feature_{i}', f'user_feature_{j}', f'user_feature_{k}'\n",
    "    val_i, val_j, val_k = target_features_49[i], target_features_49[j], target_features_49[k]\n",
    "    matching_data = item_49_data[\n",
    "        (item_49_data[feat_i] == val_i) & \n",
    "        (item_49_data[feat_j] == val_j) & \n",
    "        (item_49_data[feat_k] == val_k)\n",
    "    ]\n",
    "    if len(matching_data) > 0:\n",
    "        key = f'{feat_i}={val_i}, {feat_j}={val_j}, {feat_k}={val_k}'\n",
    "        marginal_estimates_49[key] = matching_data['click'].mean()\n",
    "        print(f\"CTR when {key}: {marginal_estimates_49[key]:.4f} (n={len(matching_data)})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MARGINAL AVERAGING ESTIMATES (Item 49):\")\n",
    "print(\"=\"*60)\n",
    "for key, val in marginal_estimates_49.items():\n",
    "    if not np.isnan(val):\n",
    "        print(f\"{key:50s}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 2: Random Forest Classifier for Item 49\n",
    "\n",
    "# Prepare features for all data\n",
    "feature_cols = ['user_feature_0', 'user_feature_1', 'user_feature_2', 'user_feature_3', 'item_id']\n",
    "\n",
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_dict_49 = {}\n",
    "X_encoded = log_df_readable[feature_cols].copy()\n",
    "\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(log_df_readable[col])\n",
    "    le_dict_49[col] = le\n",
    "\n",
    "y = log_df_readable['click']\n",
    "\n",
    "# Train Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model_49 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model_49.fit(X_encoded, y)\n",
    "\n",
    "# Create target observation for prediction\n",
    "target_obs_49 = pd.DataFrame({\n",
    "    'user_feature_0': [target_features_49[0]],\n",
    "    'user_feature_1': [target_features_49[1]],\n",
    "    'user_feature_2': [target_features_49[2]],\n",
    "    'user_feature_3': [target_features_49[3]],\n",
    "    'item_id': [target_item_49]\n",
    "})\n",
    "\n",
    "# Encode target observation\n",
    "target_encoded_49 = target_obs_49.copy()\n",
    "for col in feature_cols:\n",
    "    target_encoded_49[col] = le_dict_49[col].transform(target_obs_49[col])\n",
    "\n",
    "# Predict probability of click\n",
    "rf_pred_proba_49 = rf_model_49.predict_proba(target_encoded_49)[0, 1]\n",
    "\n",
    "print(f\"Random Forest predicted CTR for item {target_item_49} with features {target_features_49}:\")\n",
    "print(f\"  CTR = {rf_pred_proba_49:.4f}\")\n",
    "\n",
    "# Show feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for col, importance in zip(feature_cols, rf_model_49.feature_importances_):\n",
    "    print(f\"  {col:20s}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5130bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 3: Direct Method (DM) for Item 49\n",
    "\n",
    "# The DM estimator uses a reward model to predict CTR\n",
    "# We'll use the same Random Forest model as the reward model for DM\n",
    "\n",
    "dm_estimate_49 = rf_pred_proba_49  # Same as RF prediction\n",
    "\n",
    "print(f\"Direct Method (DM) estimated CTR for item {target_item_49}:\")\n",
    "print(f\"  CTR = {dm_estimate_49:.4f}\")\n",
    "print(\"\\nNote: DM uses the reward model (Random Forest) to predict expected reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 4: Doubly Robust (DR) for Item 49\n",
    "\n",
    "# For an UNOBSERVED combination, DR reduces to DM because there's no\n",
    "# observational data to provide the IPS correction term\n",
    "\n",
    "# DR formula: E[R] = E[DM(x,a)] + E[IPS * (R - DM(x,a))]\n",
    "# For unobserved: second term = 0 (no matching observations)\n",
    "\n",
    "dr_estimate_49 = dm_estimate_49  # Reduces to DM for unobserved\n",
    "\n",
    "print(f\"Doubly Robust (DR) estimated CTR for item {target_item_49}:\")\n",
    "print(f\"  CTR = {dr_estimate_49:.4f}\")\n",
    "print(\"\\nNote: For unobserved combinations, DR reduces to DM\")\n",
    "print(\"(no IPS correction possible without observed data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1091f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summary: All Methods for Item 49\n",
    "\n",
    "# Compile all estimates\n",
    "estimates_49 = {\n",
    "    'Overall Item CTR': marginal_estimates_49['overall_item'],\n",
    "    'Random Forest': rf_pred_proba_49,\n",
    "    'Direct Method (DM)': dm_estimate_49,\n",
    "    'Doubly Robust (DR)': dr_estimate_49\n",
    "}\n",
    "\n",
    "# Add marginal averages (excluding overall)\n",
    "for key, val in marginal_estimates_49.items():\n",
    "    if key != 'overall_item' and not np.isnan(val):\n",
    "        estimates_49[f'Marginal: {key}'] = val\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CTR ESTIMATES FOR ITEM {target_item_49} WITH FEATURES {target_features_49}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for method, estimate in estimates_49.items():\n",
    "    print(f\"{method:50s}: {estimate:.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization: Comparison of Estimation Methods (Item 49)\n",
    "\n",
    "# Prepare data for visualization\n",
    "main_methods_49 = {\n",
    "    'Overall\\nItem CTR': marginal_estimates_49['overall_item'],\n",
    "    'Random\\nForest': rf_pred_proba_49,\n",
    "    'Direct\\nMethod': dm_estimate_49,\n",
    "    'Doubly\\nRobust': dr_estimate_49\n",
    "}\n",
    "\n",
    "# Create comparison plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Main methods\n",
    "ax1 = axes[0]\n",
    "methods = list(main_methods_49.keys())\n",
    "values = list(main_methods_49.values())\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "bars = ax1.bar(methods, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Estimated CTR', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'CTR Estimates for Item {target_item_49}\\nUser Features: {target_features_49}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(values) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.4f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: All marginal averages\n",
    "ax2 = axes[1]\n",
    "marginal_keys = [k for k in marginal_estimates_49.keys() if not np.isnan(marginal_estimates_49[k])]\n",
    "marginal_vals = [marginal_estimates_49[k] for k in marginal_keys]\n",
    "\n",
    "# Shorten labels for readability\n",
    "short_labels = []\n",
    "for key in marginal_keys:\n",
    "    if key == 'overall_item':\n",
    "        short_labels.append('Overall')\n",
    "    else:\n",
    "        # Extract feature matches\n",
    "        short_labels.append(key.replace('user_feature_', 'F').replace(', user_feature_', ', F'))\n",
    "\n",
    "bars2 = ax2.barh(short_labels, marginal_vals, color='#9b59b6', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xlabel('Estimated CTR', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Marginal Averaging Estimates\\n(Different Feature Match Levels)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, max(marginal_vals) * 1.15)\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax2.invert_yaxis()  # Highest match level at top\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars2, marginal_vals):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "             f' {val:.4f}',\n",
    "             ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Visualization complete for item {target_item_49}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Show a specific missing combination\n",
    "if len(missing_combos) > 0:\n",
    "    example_missing = missing_combos.iloc[0]\n",
    "    print(\"Example of a missing combination:\")\n",
    "    print(example_missing.to_dict())\n",
    "    \n",
    "    # Verify it's not in the data\n",
    "    query = ' & '.join([f\"(log_df_readable['{col}'] == '{val}')\" \n",
    "                        for col, val in example_missing.to_dict().items()])\n",
    "    query += \" & (log_df_readable['item_id'] == 0)\"\n",
    "    \n",
    "    result = eval(f\"log_df_readable[{query}]\")\n",
    "    print(f\"\\nRows matching this combination with item_id=0: {len(result)}\")\n",
    "else:\n",
    "    print(\"All possible combinations are present in the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39ad01",
   "metadata": {},
   "source": [
    "### Estimating CTR for Unseen User-Item Combinations\n",
    "\n",
    "**Problem:** We want to estimate the click-through probability for item_id=0 with user features (A0, A1, A2, D3), which has never been observed in the logged data.\n",
    "\n",
    "**Approaches:**\n",
    "1. **Marginal Averaging**: Average CTR from similar partial matches\n",
    "2. **Feature-based Model**: Train a supervised model (e.g., Random Forest, Logistic Regression)\n",
    "3. **Matrix Factorization/Embeddings**: Learn latent representations\n",
    "4. **Nearest Neighbor**: Find similar observed combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unseen combination we want to estimate CTR for\n",
    "target_combo = {\n",
    "    'user_feature_0': 'A0',\n",
    "    'user_feature_1': 'A1', \n",
    "    'user_feature_2': 'A2',\n",
    "    'user_feature_3': 'D3'\n",
    "}\n",
    "target_item = 0\n",
    "\n",
    "print(f\"Target: Estimate CTR for item_id={target_item} with features:\")\n",
    "for k, v in target_combo.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Verify this combination doesn't exist for item_id=0\n",
    "query_result = log_df_readable[\n",
    "    (log_df_readable['item_id'] == target_item) &\n",
    "    (log_df_readable['user_feature_0'] == target_combo['user_feature_0']) &\n",
    "    (log_df_readable['user_feature_1'] == target_combo['user_feature_1']) &\n",
    "    (log_df_readable['user_feature_2'] == target_combo['user_feature_2']) &\n",
    "    (log_df_readable['user_feature_3'] == target_combo['user_feature_3'])\n",
    "]\n",
    "\n",
    "print(f\"\\nObservations matching this exact combination: {len(query_result)}\")\n",
    "if len(query_result) == 0:\n",
    "    print(\" Confirmed: This is an unseen combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903bd19",
   "metadata": {},
   "source": [
    "#### Approach 1: Marginal Averaging from Partial Matches\n",
    "\n",
    "Find similar combinations by matching subsets of features and average their CTRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Marginal Averaging\n",
    "# Find partial matches and compute weighted average\n",
    "\n",
    "item_0_data = log_df_readable[log_df_readable['item_id'] == target_item].copy()\n",
    "\n",
    "estimates = {}\n",
    "\n",
    "# 1. Overall average for item_id=0 (baseline)\n",
    "estimates['overall_item_avg'] = item_0_data['click'].mean()\n",
    "\n",
    "# 2. Average for each individual feature match\n",
    "for feat, val in target_combo.items():\n",
    "    mask = item_0_data[feat] == val\n",
    "    if mask.sum() > 0:\n",
    "        estimates[f'match_{feat}'] = item_0_data[mask]['click'].mean()\n",
    "        estimates[f'n_{feat}'] = mask.sum()\n",
    "\n",
    "# 3. Average for combinations of 2 features\n",
    "estimates['match_f0_f1'] = item_0_data[\n",
    "    (item_0_data['user_feature_0'] == target_combo['user_feature_0']) &\n",
    "    (item_0_data['user_feature_1'] == target_combo['user_feature_1'])\n",
    "]['click'].mean() if len(item_0_data[\n",
    "    (item_0_data['user_feature_0'] == target_combo['user_feature_0']) &\n",
    "    (item_0_data['user_feature_1'] == target_combo['user_feature_1'])\n",
    "]) > 0 else np.nan\n",
    "\n",
    "# 4. Average for combinations of 3 features  \n",
    "estimates['match_f0_f1_f2'] = item_0_data[\n",
    "    (item_0_data['user_feature_0'] == target_combo['user_feature_0']) &\n",
    "    (item_0_data['user_feature_1'] == target_combo['user_feature_1']) &\n",
    "    (item_0_data['user_feature_2'] == target_combo['user_feature_2'])\n",
    "]['click'].mean() if len(item_0_data[\n",
    "    (item_0_data['user_feature_0'] == target_combo['user_feature_0']) &\n",
    "    (item_0_data['user_feature_1'] == target_combo['user_feature_1']) &\n",
    "    (item_0_data['user_feature_2'] == target_combo['user_feature_2'])\n",
    "]) > 0 else np.nan\n",
    "\n",
    "print(\"Marginal Averaging Estimates:\")\n",
    "print(\"=\"*50)\n",
    "for key, val in estimates.items():\n",
    "    if not key.startswith('n_'):\n",
    "        print(f\"{key:30s}: {val:.4f}\" if not np.isnan(val) else f\"{key:30s}: N/A\")\n",
    "\n",
    "# Final estimate: Use the most specific available estimate\n",
    "marginal_estimate = estimates.get('match_f0_f1_f2')\n",
    "if np.isnan(marginal_estimate) or marginal_estimate is None:\n",
    "    marginal_estimate = estimates.get('match_f0_f1')\n",
    "if np.isnan(marginal_estimate) or marginal_estimate is None:\n",
    "    marginal_estimate = np.nanmean([estimates.get(f'match_{k}', np.nan) for k in target_combo.keys()])\n",
    "if np.isnan(marginal_estimate):\n",
    "    marginal_estimate = estimates['overall_item_avg']\n",
    "\n",
    "print(f\"\\n{'Final Marginal Estimate':30s}: {marginal_estimate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a2947",
   "metadata": {},
   "source": [
    "#### Approach 2: Supervised Learning Model (Random Forest)\n",
    "\n",
    "Train a model to predict CTR based on user features and item_id, then use it to predict for the unseen combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14870f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for modeling\n",
    "model_data = log_df_readable.copy()\n",
    "\n",
    "# Encode categorical features\n",
    "le_dict = {}\n",
    "for col in user_feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    model_data[f'{col}_encoded'] = le.fit_transform(model_data[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Features and target\n",
    "feature_cols = [f'{col}_encoded' for col in user_feature_cols] + ['item_id', 'position']\n",
    "X = model_data[feature_cols]\n",
    "y = model_data['click']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "train_acc = rf_model.score(X_train, y_train)\n",
    "test_acc = rf_model.score(X_test, y_test)\n",
    "print(f\"Random Forest Performance:\")\n",
    "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predict for the unseen combination\n",
    "target_features_encoded = {}\n",
    "for col in user_feature_cols:\n",
    "    try:\n",
    "        target_features_encoded[f'{col}_encoded'] = le_dict[col].transform([target_combo[col]])[0]\n",
    "    except:\n",
    "        # If the value wasn't seen during training, use the most common value\n",
    "        target_features_encoded[f'{col}_encoded'] = 0\n",
    "        print(f\"Warning: {target_combo[col]} not seen in training for {col}\")\n",
    "\n",
    "# Create prediction input for all positions\n",
    "predictions_by_position = {}\n",
    "for pos in [1, 2, 3]:\n",
    "    X_pred = pd.DataFrame([{\n",
    "        'user_feature_0_encoded': target_features_encoded['user_feature_0_encoded'],\n",
    "        'user_feature_1_encoded': target_features_encoded['user_feature_1_encoded'],\n",
    "        'user_feature_2_encoded': target_features_encoded['user_feature_2_encoded'],\n",
    "        'user_feature_3_encoded': target_features_encoded['user_feature_3_encoded'],\n",
    "        'item_id': target_item,\n",
    "        'position': pos\n",
    "    }])\n",
    "    \n",
    "    # Predict probability of click\n",
    "    pred_proba = rf_model.predict_proba(X_pred)[0, 1]  # Probability of class 1 (click)\n",
    "    predictions_by_position[f'position_{pos}'] = pred_proba\n",
    "\n",
    "print(f\"\\nRandom Forest CTR Estimates for item_id={target_item}:\")\n",
    "print(f\"  Target combination: {target_combo}\")\n",
    "for pos, prob in predictions_by_position.items():\n",
    "    print(f\"  {pos}: {prob:.4f}\")\n",
    "\n",
    "# Average across positions (if position-agnostic estimate needed)\n",
    "rf_estimate = np.mean(list(predictions_by_position.values()))\n",
    "print(f\"\\n{'Final RF Estimate (avg)':30s}: {rf_estimate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Compare all estimates\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY: CTR Estimates for Unseen Combination\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Item ID: {target_item}\")\n",
    "print(f\"User Features: {target_combo}\")\n",
    "print(\"\\nEstimates:\")\n",
    "print(f\"  1. Marginal Averaging: {marginal_estimate:.4f}\")\n",
    "print(f\"  2. Random Forest:      {rf_estimate:.4f}\")\n",
    "print(f\"\\n  Baseline (overall item avg): {estimates['overall_item_avg']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Marginal Averaging', 'Random Forest', 'Baseline (Item Avg)'],\n",
    "    'Estimated_CTR': [marginal_estimate, rf_estimate, estimates['overall_item_avg']]\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig = px.bar(comparison_df, \n",
    "             x='Method', \n",
    "             y='Estimated_CTR',\n",
    "             title=f'CTR Estimates for Unseen Combination (item_id={target_item})',\n",
    "             labels={'Estimated_CTR': 'Estimated CTR'},\n",
    "             text='Estimated_CTR',\n",
    "             height=400)\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a3538",
   "metadata": {},
   "source": [
    "#### Approach 3: Offline Bandit Evaluation Methods\n",
    "\n",
    "Use proper OPE (Offline Policy Evaluation) estimators that leverage propensity scores:\n",
    "- **IPS (Inverse Propensity Scoring)**: Reweights logged data by inverse propensity\n",
    "- **DR (Doubly Robust)**: Combines IPS with a reward model for lower variance\n",
    "- **DM (Direct Method)**: Uses a learned reward model only\n",
    "\n",
    "These methods are designed specifically for counterfactual estimation in contextual bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0931be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build a regression model to estimate E[r|x,a] (expected reward given context and action)\n",
    "# This will be used for Direct Method (DM) and Doubly Robust (DR) estimators\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Prepare data - use the same encoding as before\n",
    "reward_model_data = log_df_readable.copy()\n",
    "\n",
    "# Use the already encoded features from before\n",
    "for col in user_feature_cols:\n",
    "    reward_model_data[f'{col}_encoded'] = le_dict[col].transform(reward_model_data[col].astype(str))\n",
    "\n",
    "# Features: user features + item_id + position\n",
    "reward_feature_cols = [f'{col}_encoded' for col in user_feature_cols] + ['item_id', 'position']\n",
    "X_reward = reward_model_data[reward_feature_cols]\n",
    "y_reward = reward_model_data['click']\n",
    "\n",
    "# Train reward model (regression for expected reward)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reward, y_reward, test_size=0.2, random_state=42)\n",
    "\n",
    "reward_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "reward_model.fit(X_train_r, y_train_r)\n",
    "\n",
    "train_r2 = reward_model.score(X_train_r, y_train_r)\n",
    "test_r2 = reward_model.score(X_test_r, y_test_r)\n",
    "\n",
    "print(f\"Reward Model Performance (R):\")\n",
    "print(f\"  Train: {train_r2:.4f}\")\n",
    "print(f\"  Test:  {test_r2:.4f}\")\n",
    "\n",
    "# Get estimated rewards for all data (q-hat)\n",
    "reward_model_data['estimated_reward'] = reward_model.predict(X_reward)\n",
    "\n",
    "print(f\"\\nEstimated reward distribution:\")\n",
    "print(reward_model_data['estimated_reward'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Method (DM): Simply use the reward model prediction\n",
    "# E[r|x,a] where x = target_combo, a = target_item\n",
    "\n",
    "dm_estimates = {}\n",
    "for pos in [1, 2, 3]:\n",
    "    X_target = pd.DataFrame([{\n",
    "        'user_feature_0_encoded': target_features_encoded['user_feature_0_encoded'],\n",
    "        'user_feature_1_encoded': target_features_encoded['user_feature_1_encoded'],\n",
    "        'user_feature_2_encoded': target_features_encoded['user_feature_2_encoded'],\n",
    "        'user_feature_3_encoded': target_features_encoded['user_feature_3_encoded'],\n",
    "        'item_id': target_item,\n",
    "        'position': pos\n",
    "    }])\n",
    "    \n",
    "    dm_estimates[f'position_{pos}'] = reward_model.predict(X_target)[0]\n",
    "\n",
    "print(f\"Direct Method (DM) Estimates:\")\n",
    "print(f\"  Target: item_id={target_item}, features={target_combo}\")\n",
    "for pos, est in dm_estimates.items():\n",
    "    print(f\"  {pos}: {est:.4f}\")\n",
    "\n",
    "dm_estimate = np.mean(list(dm_estimates.values()))\n",
    "print(f\"\\n{'Final DM Estimate (avg)':30s}: {dm_estimate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ecd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doubly Robust (DR) Estimator\n",
    "# For unseen combinations, we can't use IPS directly (no logged data with this exact combo)\n",
    "# But we can understand how DR would work if we had similar data\n",
    "\n",
    "# DR formula: E[V()] = E[q(x,a) + (r - q(x,a)) * (a|x) / _b(a|x)]\n",
    "# where:\n",
    "#   - q(x,a) is the estimated reward model (already have this)\n",
    "#   - r is the actual reward (observed)\n",
    "#   - (a|x) is the evaluation policy (what we'd like to evaluate)\n",
    "#   - _b(a|x) is the behavior policy (propensity scores we have)\n",
    "\n",
    "# For our unseen combination, we only have q(x,a) since there's no logged data\n",
    "# So DR reduces to DM for completely unseen combinations\n",
    "\n",
    "print(\"Doubly Robust (DR) Estimator:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"For UNSEEN combinations (like our target):\")\n",
    "print(f\"  - No logged data exists  IPS term = 0\")\n",
    "print(f\"  - DR reduces to Direct Method (DM)\")\n",
    "print(f\"  - Estimate = q(x,a) = {dm_estimate:.4f}\")\n",
    "print()\n",
    "print(f\"For SEEN combinations:\")\n",
    "print(f\"  - DR = q(x,a) + (r - q(x,a)) * (a|x) / _b(a|x)\")\n",
    "print(f\"  - Combines model prediction with importance-weighted residuals\")\n",
    "print(f\"  - Lower variance than pure IPS, less biased than pure DM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: Show DR for a SEEN combination for comparison\n",
    "seen_example = item_0_data.iloc[0]  # First row with item_id=0\n",
    "\n",
    "# Get the reward model prediction for this seen example\n",
    "seen_features = {\n",
    "    'user_feature_0_encoded': le_dict['user_feature_0'].transform([seen_example['user_feature_0']])[0],\n",
    "    'user_feature_1_encoded': le_dict['user_feature_1'].transform([seen_example['user_feature_1']])[0],\n",
    "    'user_feature_2_encoded': le_dict['user_feature_2'].transform([seen_example['user_feature_2']])[0],\n",
    "    'user_feature_3_encoded': le_dict['user_feature_3'].transform([seen_example['user_feature_3']])[0],\n",
    "    'item_id': int(seen_example['item_id']),\n",
    "    'position': int(seen_example['position'])\n",
    "}\n",
    "\n",
    "X_seen = pd.DataFrame([seen_features])\n",
    "q_hat = reward_model.predict(X_seen)[0]\n",
    "actual_reward = seen_example['click']\n",
    "propensity = seen_example['propensity_score']\n",
    "\n",
    "# Assuming evaluation policy would always select this item ((a|x) = 1)\n",
    "eval_policy_prob = 1.0\n",
    "\n",
    "# DR estimate for this one example\n",
    "dr_contribution = q_hat + (actual_reward - q_hat) * (eval_policy_prob / propensity)\n",
    "\n",
    "print(f\"\\nExample DR calculation for a SEEN combination:\")\n",
    "print(f\"  Features: {seen_example[user_feature_cols].to_dict()}\")\n",
    "print(f\"  q(x,a):         {q_hat:.4f}\")\n",
    "print(f\"  actual reward:  {actual_reward:.4f}\")\n",
    "print(f\"  propensity:     {propensity:.4f}\")\n",
    "print(f\"  DR estimate:    {dr_contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OBP's OffPolicyEvaluation framework\n",
    "# This demonstrates how to evaluate a NEW policy that prefers our target combination\n",
    "\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    InverseProbabilityWeighting as IPW,\n",
    "    DirectMethod as DM_OBP,\n",
    "    DoublyRobust as DR_OBP,\n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "print(\"Using OBP for Policy Evaluation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create action distributions for evaluation policy\n",
    "# We'll create a policy that has higher probability for item_id=0 with our target features\n",
    "\n",
    "# For OBP, we need action_dist in format: (n_rounds, n_actions, len_list)\n",
    "# But for this demo, let's show the conceptual approach\n",
    "\n",
    "print(\"\\nKey Insights for Unseen Combinations:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. IPS (Inverse Propensity Scoring):\")\n",
    "print(\"   - Requires: logged data with _b(a|x) > 0\")\n",
    "print(\"   - Cannot estimate for unseen combinations directly\")\n",
    "print(\"   - Unbiased but high variance\")\n",
    "print()\n",
    "print(\"2. Direct Method (DM):\")\n",
    "print(\"   - Uses: q(x,a) from regression model\")\n",
    "print(\"   - CAN estimate for unseen combinations \")\n",
    "print(\"   - Biased if model is wrong, but lower variance\")\n",
    "print(f\"   - Our estimate: {dm_estimate:.4f}\")\n",
    "print()\n",
    "print(\"3. Doubly Robust (DR):\")\n",
    "print(\"   - Combines: q(x,a) + IPS correction\")\n",
    "print(\"   - For unseen: reduces to DM (no IPS correction possible)\")\n",
    "print(f\"   - Our estimate: {dm_estimate:.4f}\")\n",
    "print()\n",
    "print(\"4. Propensity Scores:\")\n",
    "print(f\"   - Behavior policy propensity for item {target_item}:\")\n",
    "print(f\"     Average: {log_df_readable[log_df_readable['item_id']==target_item]['propensity_score'].mean():.4f}\")\n",
    "print(f\"     But for our EXACT combo: 0 (never shown)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1059f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE COMPARISON: All Methods\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: item_id={target_item}\")\n",
    "print(f\"Features: {target_combo}\")\n",
    "print(f\"Status: UNSEEN combination (0 logged observations)\")\n",
    "print()\n",
    "print(f\"{'Method':<30} {'Estimate':<12} {'Notes'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'1. Marginal Averaging':<30} {marginal_estimate:>10.4f}   Heuristic\")\n",
    "print(f\"{'2. Random Forest (Direct)':<30} {rf_estimate:>10.4f}   Supervised ML\")\n",
    "print(f\"{'3. DM (Reward Model)':<30} {dm_estimate:>10.4f}   Model-based\")\n",
    "print(f\"{'4. DR (reduces to DM)':<30} {dm_estimate:>10.4f}   No IPS term\")\n",
    "print(f\"{'5. Baseline (Item Avg)':<30} {estimates['overall_item_avg']:>10.4f}   Simple avg\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Recommendations:\")\n",
    "print(\" Use DM/DR with a good reward model for unseen combinations\")\n",
    "print(\" Marginal averaging is interpretable but may miss interactions\")  \n",
    "print(\" For SEEN combinations, DR > IPS > DM (typically)\")\n",
    "print(\" For UNSEEN combinations, DM is the only propensity-based option\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Updated visualization\n",
    "comparison_df_full = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Marginal\\nAveraging', \n",
    "        'Random Forest\\n(Classification)',\n",
    "        'Direct Method\\n(Regression)',\n",
    "        'Doubly Robust\\n(DRDM)',\n",
    "        'Baseline\\n(Item Avg)'\n",
    "    ],\n",
    "    'Estimated_CTR': [\n",
    "        marginal_estimate, \n",
    "        rf_estimate, \n",
    "        dm_estimate,\n",
    "        dm_estimate,  # Same as DM for unseen\n",
    "        estimates['overall_item_avg']\n",
    "    ],\n",
    "    'Type': [\n",
    "        'Heuristic',\n",
    "        'Supervised ML',\n",
    "        'OPE (Model)',\n",
    "        'OPE (Hybrid)',\n",
    "        'Baseline'\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig = px.bar(comparison_df_full, \n",
    "             x='Method', \n",
    "             y='Estimated_CTR',\n",
    "             color='Type',\n",
    "             title=f'CTR Estimates for Unseen Combination<br>item_id={target_item}, features={target_combo}',\n",
    "             labels={'Estimated_CTR': 'Estimated CTR'},\n",
    "             text='Estimated_CTR',\n",
    "             height=500)\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')\n",
    "fig.update_layout(showlegend=True, xaxis_tickangle=-15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9919fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df_readable[log_df_readable['item_id'] == 0].groupby('propensity_score').size().reset_index(name='count').sort_values('propensity_score').sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ebde5",
   "metadata": {},
   "source": [
    "### Manual Propensity Score Computation\n",
    "\n",
    "This function computes manual propensity scores based on the empirical frequency of (item, categorical_feature) pairs in the logged data.\n",
    "\n",
    "**Use cases:**\n",
    "- Compute propensity scores for any combination of item_id and categorical features (user_feature_0, position, etc.)\n",
    "- The propensity represents P(item, feature) based on observed counts\n",
    "- Can be used to understand the logging policy's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27691c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64fb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789584f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(log_df_readable[log_df_readable['item_id'] == 0]['propensity_score'], bins=30, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df_readable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21750b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.groupby(\"item_id\")[\"propensity_score\"].agg([\"mean\", \"std\"]).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
